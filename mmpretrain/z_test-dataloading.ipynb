{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03871870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert.breslin/miniconda3/envs/venv_sebnet/lib/python3.8/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner import Runner\n",
    "from mmengine.registry import DATASETS\n",
    "from mmengine.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456e6039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config (path: configs/test.py): {'model': {'type': 'ImageClassifier', 'backbone': {'type': 'ResNet', 'depth': 50, 'num_stages': 4, 'out_indices': (3,), 'style': 'pytorch', 'frozen_stages': -1}, 'neck': {'type': 'GlobalAveragePooling'}, 'head': {'type': 'LinearClsHead', 'num_classes': 1000, 'in_channels': 2048, 'loss': {'type': 'CrossEntropyLoss', 'loss_weight': 1.0}, 'topk': (1, 5)}}, 'dataset_type': 'ImageNet', 'data_preprocessor': {'num_classes': 1000, 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, 'train_pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'RandomResizedCrop', 'scale': 224}, {'type': 'RandomFlip', 'prob': 0.5, 'direction': 'horizontal'}, {'type': 'PackInputs'}], 'test_pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'ResizeEdge', 'scale': 256, 'edge': 'short'}, {'type': 'CenterCrop', 'crop_size': 224}, {'type': 'PackInputs'}], 'train_dataloader': {'batch_size': 32, 'num_workers': 5, 'dataset': {'type': 'ImageNet', 'data_root': 'data/imagenet', 'split': 'train', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'RandomResizedCrop', 'scale': 224}, {'type': 'RandomFlip', 'prob': 0.5, 'direction': 'horizontal'}, {'type': 'PackInputs'}], 'ann_file': 'meta/train.txt', 'data_prefix': 'train'}, 'sampler': {'type': 'DefaultSampler', 'shuffle': True}, 'persistent_workers': True}, 'val_dataloader': {'batch_size': 32, 'num_workers': 5, 'dataset': {'type': 'ImageNet', 'data_root': 'data/imagenet', 'split': 'val', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'ResizeEdge', 'scale': 256, 'edge': 'short'}, {'type': 'CenterCrop', 'crop_size': 224}, {'type': 'PackInputs'}], 'ann_file': 'meta/val.txt', 'data_prefix': 'val'}, 'sampler': {'type': 'DefaultSampler', 'shuffle': False}, 'persistent_workers': True}, 'val_evaluator': {'type': 'Accuracy', 'topk': (1, 5)}, 'test_dataloader': {'batch_size': 32, 'num_workers': 5, 'dataset': {'type': 'ImageNet', 'data_root': 'data/imagenet', 'split': 'val', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'ResizeEdge', 'scale': 256, 'edge': 'short'}, {'type': 'CenterCrop', 'crop_size': 224}, {'type': 'PackInputs'}], 'ann_file': 'meta/val.txt', 'data_prefix': 'val'}, 'sampler': {'type': 'DefaultSampler', 'shuffle': False}, 'persistent_workers': True}, 'test_evaluator': {'type': 'Accuracy', 'topk': (1, 5)}, 'optim_wrapper': {'optimizer': {'type': 'SGD', 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001}}, 'param_scheduler': {'type': 'MultiStepLR', 'by_epoch': True, 'milestones': [30, 60, 90], 'gamma': 0.1}, 'train_cfg': {'by_epoch': True, 'max_epochs': 100, 'val_interval': 1}, 'val_cfg': {}, 'test_cfg': {}, 'auto_scale_lr': {'base_batch_size': 256}, 'default_scope': 'mmpretrain', 'default_hooks': {'timer': {'type': 'IterTimerHook'}, 'logger': {'type': 'LoggerHook', 'interval': 100}, 'param_scheduler': {'type': 'ParamSchedulerHook'}, 'checkpoint': {'type': 'CheckpointHook', 'interval': 1, 'save_begin': 74}, 'sampler_seed': {'type': 'DistSamplerSeedHook'}, 'visualization': {'type': 'VisualizationHook', 'enable': False}}, 'env_cfg': {'cudnn_benchmark': False, 'mp_cfg': {'mp_start_method': 'fork', 'opencv_num_threads': 0}, 'dist_cfg': {'backend': 'nccl'}}, 'vis_backends': [{'type': 'LocalVisBackend'}], 'visualizer': {'type': 'UniversalVisualizer', 'vis_backends': [{'type': 'LocalVisBackend'}], 'name': 'visualizer'}, 'log_level': 'INFO', 'load_from': None, 'resume': False, 'randomness': {'seed': None, 'deterministic': False}, 'work_dir': 'z_test_work_dir'}\n",
      "06/19 14:09:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 2109001777\n",
      "    GPU 0,1,2,3: NVIDIA A100-SXM4-80GB\n",
      "    CUDA_HOME: /usr\n",
      "    NVCC: Cuda compilation tools, release 10.1, V10.1.24\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 2.4.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 12.1\n",
      "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 90.1  (built against CUDA 12.4)\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
      "\n",
      "    TorchVision: 0.19.1\n",
      "    OpenCV: 4.11.0\n",
      "    MMEngine: 0.10.7\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 2109001777\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "06/19 14:09:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=256)\n",
      "data_preprocessor = dict(\n",
      "    mean=[\n",
      "        123.675,\n",
      "        116.28,\n",
      "        103.53,\n",
      "    ],\n",
      "    num_classes=1000,\n",
      "    std=[\n",
      "        58.395,\n",
      "        57.12,\n",
      "        57.375,\n",
      "    ],\n",
      "    to_rgb=True)\n",
      "dataset_type = 'ImageNet'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, save_begin=74, type='CheckpointHook'),\n",
      "    logger=dict(interval=100, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(enable=False, type='VisualizationHook'))\n",
      "default_scope = 'mmpretrain'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=50,\n",
      "        frozen_stages=-1,\n",
      "        num_stages=4,\n",
      "        out_indices=(3, ),\n",
      "        style='pytorch',\n",
      "        type='ResNet'),\n",
      "    head=dict(\n",
      "        in_channels=2048,\n",
      "        loss=dict(loss_weight=1.0, type='CrossEntropyLoss'),\n",
      "        num_classes=1000,\n",
      "        topk=(\n",
      "            1,\n",
      "            5,\n",
      "        ),\n",
      "        type='LinearClsHead'),\n",
      "    neck=dict(type='GlobalAveragePooling'),\n",
      "    type='ImageClassifier')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(lr=0.1, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
      "param_scheduler = dict(\n",
      "    by_epoch=True, gamma=0.1, milestones=[\n",
      "        30,\n",
      "        60,\n",
      "        90,\n",
      "    ], type='MultiStepLR')\n",
      "randomness = dict(deterministic=False, seed=None)\n",
      "resume = False\n",
      "test_cfg = dict()\n",
      "test_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    dataset=dict(\n",
      "        ann_file='meta/val.txt',\n",
      "        data_prefix='val',\n",
      "        data_root='data/imagenet',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(edge='short', scale=256, type='ResizeEdge'),\n",
      "            dict(crop_size=224, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        split='val',\n",
      "        type='ImageNet'),\n",
      "    num_workers=5,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    topk=(\n",
      "        1,\n",
      "        5,\n",
      "    ), type='Accuracy')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(edge='short', scale=256, type='ResizeEdge'),\n",
      "    dict(crop_size=224, type='CenterCrop'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "train_cfg = dict(by_epoch=True, max_epochs=100, val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    dataset=dict(\n",
      "        ann_file='meta/train.txt',\n",
      "        data_prefix='train',\n",
      "        data_root='data/imagenet',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=224, type='RandomResizedCrop'),\n",
      "            dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        split='train',\n",
      "        type='ImageNet'),\n",
      "    num_workers=5,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=224, type='RandomResizedCrop'),\n",
      "    dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "val_cfg = dict()\n",
      "val_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    dataset=dict(\n",
      "        ann_file='meta/val.txt',\n",
      "        data_prefix='val',\n",
      "        data_root='data/imagenet',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(edge='short', scale=256, type='ResizeEdge'),\n",
      "            dict(crop_size=224, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        split='val',\n",
      "        type='ImageNet'),\n",
      "    num_workers=5,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    topk=(\n",
      "        1,\n",
      "        5,\n",
      "    ), type='Accuracy')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='UniversalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = 'z_test_work_dir'\n",
      "\n",
      "06/19 14:09:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "06/19 14:09:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    }
   ],
   "source": [
    "# Load your config\n",
    "cfg = Config.fromfile('configs/test.py')\n",
    "print(cfg)\n",
    "runner = Runner.from_cfg(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3b236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1:\n",
      "Sample: \n",
      "{'inputs': tensor([[[255, 255, 255,  ...,  43,  44,  45],\n",
      "         [255, 255, 255,  ...,  45,  46,  47],\n",
      "         [255, 255, 255,  ...,  43,  44,  45],\n",
      "         ...,\n",
      "         [255, 255, 255,  ...,  21,  21,  21],\n",
      "         [255, 255, 255,  ...,  17,  19,  17],\n",
      "         [255, 255, 255,  ...,  18,  19,  17]],\n",
      "\n",
      "        [[255, 255, 255,  ...,  87,  90,  93],\n",
      "         [255, 255, 255,  ...,  88,  91,  95],\n",
      "         [255, 255, 255,  ...,  86,  89,  93],\n",
      "         ...,\n",
      "         [255, 255, 255,  ...,  69,  72,  73],\n",
      "         [255, 255, 255,  ...,  70,  73,  72],\n",
      "         [255, 255, 255,  ...,  73,  76,  75]],\n",
      "\n",
      "        [[255, 255, 255,  ...,  91,  94,  97],\n",
      "         [255, 255, 255,  ...,  94,  97, 101],\n",
      "         [255, 255, 255,  ...,  93,  96,  99],\n",
      "         ...,\n",
      "         [255, 255, 255,  ...,  56,  58,  59],\n",
      "         [255, 255, 255,  ...,  51,  53,  53],\n",
      "         [255, 255, 255,  ...,  50,  52,  51]]], dtype=torch.uint8), 'data_samples': <DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (224, 224, 3)\n",
      "    img_path: data/imagenet/train/n01440764/n01440764_10026.JPEG\n",
      "    flip: True\n",
      "    ori_shape: (250, 250)\n",
      "    sample_idx: 0\n",
      "    flip_direction: horizontal\n",
      "\n",
      "DATA FIELDS\n",
      "    gt_label: tensor([0])\n",
      "\n",
      ") at 0x7f9143633220>}\n",
      "--------------------------------------------------\n",
      "Image 2:\n",
      "Sample: \n",
      "{'inputs': tensor([[[139, 138, 135,  ..., 254, 254, 254],\n",
      "         [145, 144, 143,  ..., 254, 254, 254],\n",
      "         [155, 155, 155,  ..., 254, 254, 254],\n",
      "         ...,\n",
      "         [ 44,  41,  33,  ..., 153, 153, 153],\n",
      "         [ 58,  59,  59,  ..., 151, 152, 152],\n",
      "         [ 68,  70,  75,  ..., 151, 152, 152]],\n",
      "\n",
      "        [[159, 158, 156,  ..., 254, 254, 254],\n",
      "         [167, 167, 166,  ..., 254, 254, 254],\n",
      "         [181, 181, 181,  ..., 254, 254, 254],\n",
      "         ...,\n",
      "         [ 56,  53,  45,  ..., 163, 163, 163],\n",
      "         [ 71,  71,  71,  ..., 161, 162, 162],\n",
      "         [ 80,  82,  87,  ..., 161, 162, 162]],\n",
      "\n",
      "        [[230, 230, 229,  ..., 254, 254, 254],\n",
      "         [236, 236, 236,  ..., 254, 254, 254],\n",
      "         [246, 246, 246,  ..., 254, 254, 254],\n",
      "         ...,\n",
      "         [ 60,  57,  49,  ..., 170, 170, 170],\n",
      "         [ 75,  75,  75,  ..., 168, 169, 169],\n",
      "         [ 84,  86,  91,  ..., 168, 169, 169]]], dtype=torch.uint8), 'data_samples': <DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (224, 224, 3)\n",
      "    img_path: data/imagenet/train/n01440764/n01440764_10027.JPEG\n",
      "    flip: False\n",
      "    ori_shape: (150, 200)\n",
      "    sample_idx: 1\n",
      "    flip_direction: None\n",
      "\n",
      "DATA FIELDS\n",
      "    gt_label: tensor([0])\n",
      "\n",
      ") at 0x7f914dba7af0>}\n",
      "--------------------------------------------------\n",
      "Image 3:\n",
      "Sample: \n",
      "{'inputs': tensor([[[158, 152, 143,  ..., 112, 111, 111],\n",
      "         [163, 156, 145,  ..., 110, 110, 110],\n",
      "         [167, 162, 153,  ..., 107, 107, 107],\n",
      "         ...,\n",
      "         [ 85,  82,  77,  ...,  74,  73,  72],\n",
      "         [ 87,  84,  80,  ...,  72,  72,  71],\n",
      "         [ 87,  86,  85,  ...,  69,  70,  71]],\n",
      "\n",
      "        [[190, 184, 173,  ..., 105, 103, 102],\n",
      "         [194, 186, 173,  ..., 102, 102, 102],\n",
      "         [197, 191, 180,  ...,  99, 100, 100],\n",
      "         ...,\n",
      "         [114, 113, 110,  ..., 107, 105, 104],\n",
      "         [111, 111, 112,  ..., 104, 104, 103],\n",
      "         [108, 111, 116,  ..., 101, 102, 103]],\n",
      "\n",
      "        [[209, 203, 192,  ..., 109, 106, 105],\n",
      "         [213, 206, 193,  ..., 108, 107, 106],\n",
      "         [216, 210, 200,  ..., 107, 106, 105],\n",
      "         ...,\n",
      "         [ 97,  96,  93,  ...,  88,  86,  85],\n",
      "         [ 95,  96,  95,  ...,  85,  85,  84],\n",
      "         [ 93,  95,  99,  ...,  82,  83,  84]]], dtype=torch.uint8), 'data_samples': <DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (224, 224, 3)\n",
      "    img_path: data/imagenet/train/n01440764/n01440764_10029.JPEG\n",
      "    flip: False\n",
      "    ori_shape: (375, 500)\n",
      "    sample_idx: 2\n",
      "    flip_direction: None\n",
      "\n",
      "DATA FIELDS\n",
      "    gt_label: tensor([0])\n",
      "\n",
      ") at 0x7f914dac34c0>}\n",
      "--------------------------------------------------\n",
      "Image 4:\n",
      "Sample: \n",
      "{'inputs': tensor([[[205, 200, 201,  ..., 255, 255, 255],\n",
      "         [200, 203, 200,  ..., 255, 254, 255],\n",
      "         [186, 199, 201,  ..., 255, 255, 255],\n",
      "         ...,\n",
      "         [  7,   7,   9,  ...,  47,  51,  49],\n",
      "         [  8,   6,   9,  ...,  43,  49,  52],\n",
      "         [  9,   5,   7,  ...,  49,  44,  48]],\n",
      "\n",
      "        [[190, 192, 193,  ..., 245, 245, 245],\n",
      "         [191, 193, 197,  ..., 245, 244, 245],\n",
      "         [187, 192, 195,  ..., 246, 245, 245],\n",
      "         ...,\n",
      "         [  8,   8,  10,  ...,  99, 106, 100],\n",
      "         [ 10,   7,  10,  ..., 100, 102, 108],\n",
      "         [ 10,   7,   8,  ..., 108, 101, 100]],\n",
      "\n",
      "        [[183, 183, 184,  ..., 238, 238, 238],\n",
      "         [179, 184, 187,  ..., 238, 237, 238],\n",
      "         [173, 182, 184,  ..., 239, 238, 238],\n",
      "         ...,\n",
      "         [  6,   6,   8,  ...,  89,  96,  92],\n",
      "         [  7,   5,   8,  ...,  90,  92,  97],\n",
      "         [  8,   5,   6,  ...,  97,  90,  92]]], dtype=torch.uint8), 'data_samples': <DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (224, 224, 3)\n",
      "    img_path: data/imagenet/train/n01440764/n01440764_10040.JPEG\n",
      "    flip: False\n",
      "    ori_shape: (375, 500)\n",
      "    sample_idx: 3\n",
      "    flip_direction: None\n",
      "\n",
      "DATA FIELDS\n",
      "    gt_label: tensor([0])\n",
      "\n",
      ") at 0x7f9143593490>}\n",
      "--------------------------------------------------\n",
      "Image 5:\n",
      "Sample: \n",
      "{'inputs': tensor([[[ 49,  49,  50,  ...,  74,  73,  73],\n",
      "         [ 56,  57,  59,  ...,  77,  68,  64],\n",
      "         [ 65,  67,  71,  ...,  79,  61,  53],\n",
      "         ...,\n",
      "         [ 39,  42,  50,  ...,  85,  91,  94],\n",
      "         [ 34,  37,  44,  ...,  65,  72,  74],\n",
      "         [ 30,  33,  40,  ...,  51,  57,  59]],\n",
      "\n",
      "        [[ 70,  72,  77,  ..., 153, 151, 150],\n",
      "         [ 76,  79,  85,  ..., 153, 144, 140],\n",
      "         [ 83,  87,  95,  ..., 154, 136, 127],\n",
      "         ...,\n",
      "         [ 53,  54,  58,  ..., 126, 133, 136],\n",
      "         [ 50,  51,  53,  ..., 101, 108, 111],\n",
      "         [ 48,  48,  48,  ...,  83,  89,  92]],\n",
      "\n",
      "        [[ 55,  57,  61,  ..., 144, 143, 142],\n",
      "         [ 61,  63,  69,  ..., 145, 136, 132],\n",
      "         [ 68,  72,  80,  ..., 146, 128, 120],\n",
      "         ...,\n",
      "         [ 52,  53,  57,  ..., 125, 132, 136],\n",
      "         [ 49,  50,  52,  ..., 102, 109, 113],\n",
      "         [ 47,  47,  48,  ...,  86,  92,  95]]], dtype=torch.uint8), 'data_samples': <DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (224, 224, 3)\n",
      "    img_path: data/imagenet/train/n01440764/n01440764_10042.JPEG\n",
      "    flip: True\n",
      "    ori_shape: (202, 153)\n",
      "    sample_idx: 4\n",
      "    flip_direction: horizontal\n",
      "\n",
      "DATA FIELDS\n",
      "    gt_label: tensor([0])\n",
      "\n",
      ") at 0x7f91114260d0>}\n",
      "--------------------------------------------------\n",
      "Sample: \n",
      "{'inputs': tensor([[[180, 149, 130,  ..., 165, 140, 112],\n",
      "         [181, 149, 135,  ..., 175, 137,  71],\n",
      "         [178, 133, 113,  ..., 178, 162, 171],\n",
      "         ...,\n",
      "         [172, 141, 145,  ..., 137, 117,  83],\n",
      "         [128, 157, 139,  ..., 139, 108, 133],\n",
      "         [118, 126, 113,  ..., 146, 120, 117]],\n",
      "\n",
      "        [[212, 166, 154,  ..., 194, 163, 139],\n",
      "         [211, 172, 165,  ..., 199, 159, 103],\n",
      "         [204, 161, 144,  ..., 199, 197, 214],\n",
      "         ...,\n",
      "         [198, 159, 171,  ..., 147, 132,  93],\n",
      "         [142, 165, 163,  ..., 148, 122, 157],\n",
      "         [124, 139, 138,  ..., 159, 134, 144]],\n",
      "\n",
      "        [[232, 184, 171,  ..., 218, 176, 158],\n",
      "         [233, 185, 178,  ..., 226, 174, 125],\n",
      "         [226, 184, 161,  ..., 212, 218, 233],\n",
      "         ...,\n",
      "         [214, 179, 194,  ..., 141, 126,  90],\n",
      "         [151, 182, 185,  ..., 147, 116, 161],\n",
      "         [127, 152, 158,  ..., 164, 133, 154]]], dtype=torch.uint8), 'data_samples': <DataSample(\n",
      "\n",
      "META INFORMATION\n",
      "    img_shape: (224, 224, 3)\n",
      "    img_path: data/imagenet/train/n01775062/n01775062_3267.JPEG\n",
      "    flip: False\n",
      "    ori_shape: (375, 500)\n",
      "    sample_idx: 100000\n",
      "    flip_direction: None\n",
      "\n",
      "DATA FIELDS\n",
      "    gt_label: tensor([77])\n",
      "\n",
      ") at 0x7f914e14a2e0>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Access the dataset directly\n",
    "train_dataset = runner.train_dataloader.dataset\n",
    "\n",
    "# Inspect first 5 samples\n",
    "for i in range(5):\n",
    "    sample = train_dataset[i]\n",
    "    print(f\"Image {i+1}:\")\n",
    "    print(f\"Sample: \\n{sample}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "sample = train_dataset[100000]\n",
    "print(f\"Sample: \\n{sample}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sebnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
